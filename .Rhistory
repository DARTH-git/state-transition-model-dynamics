return(gof_score)
}
prior <- function(theta) {
## This is arbitrary but we just say that all samples are equally likely but we are uncertain
## so they all have a low likelihood
## we would need to modify this a bit if we wanted to be super correct about
## specifying the prior likelihood but it is good enough for now
if (is.matrix(theta)) {
ppp = theta[, 1]
ppp = rep(1/100000, length(ppp))
}
else {
ppp = 1/100000
}
return(ppp)
}
sample.prior <- function(n) {
## This is directly analogous to our sampling n parameter sets from our prior distributions in
## all of the other calibrations
rand_matrix = make_random_search_matrix(n)
rand_matrix <-rand_matrix[, -2]
return(rand_matrix)
}
## HERE WE RUN THE IMIS
## Our initial samples will be 10*B (By how the IMIS is implemented in the R package)
## Each subsequent sample will have B=5000 parameter sets drawn from near
## the best fitting set at each iteration
## The number of iterations is number_k = 50
## We will get a final resample of 200000
## At none of the iterations will we use built in optimizers (D=0)
## instead the algorithm uses a multivariate normal centered around the best fitting set
## with covariance matrix estimated from nearby samples (mahalanobis distance) weighted by their
## likelihood
result = IMIS(B = 5000, B.re = 200000, number_k = 50, D = 0)
## We add in our probability of Helathy -> Dead = 0.005 to all parameter sets
fixed_col = rep(0.005, length(result$resample[, 1]))
result_2 = cbind(result$resample[, 1], fixed_col, result$resample[, 2], result$resample[, 3])
## We rename the data set so that it is consistent with our past calibrations
rand_all_data_5 = result_2
par(mfrow= c (1, 3), oma = c(0,0,2,0))
plot(density(x = rand_all_data_5[, 1]), main = "Posterior HS")
plot(density(x = rand_all_data_5[, 3]), main = "Posterior SH")
plot(density(x = rand_all_data_5[, 4]), main = "Posterior hrr D in S")
mtext("Results of IMIS Calibration", outer = TRUE, cex = 1.5)
graph_outcomes(rand_all_data_5, "Full Results of IMIS Calibration")
compare_results(rand_all_data_5, rand_good_fits_4, "Comparing IMIS to Likelihood-based acceptance sampling 100000")
par(mfrow = c(2, 3))
plot(density(x = rand_all_data_5[, 1]), main = "Posterior HS")
plot(density(x = rand_all_data_5[, 3]), main = "Posterior SH")
plot(density(x = rand_all_data_5[, 4]), main = "Posterior hrr D in S")
plot(density(x = samples_scores_4[, 1], weights = www), main = "Posterior HS")
plot(density(x = samples_scores_4[, 3], weights = www), main = "Posterior SH")
plot(density(x = samples_scores_4[, 4], weights = www), main = "Posterior hrr D in S")
mtext("Comparing IMIS to SIR Calibration", outer = TRUE, cex = 1.5)
# STEP 10: DIRECTED SEARCH APPROACHES (NLM AS EXAMPLE) =====================================
## NELDER MEAD CALIBRATION
## First we write our directed search NM function that uses our
## likelihood-based scoring functions
nm_score_params_likelihood <- function(params) {
## params defines the starting parameter set that the NM algorithm will begin at
## lower and upper define the ranges over which the NM algorithm is allowed to search (our bounds)
## we call the neldermead function to look for better fitting sets
## we return the set the algorithm identifies
## note that the results may not have converged and we will test that outside of this function
lower <- c(0, 0.005, 0, 1)
upper <- c(0.5, 0.005, 0.5, 10)
nm_result = neldermead(params, score_params_likelihood, lower, upper)
return(nm_result)
}
# We start our search repeatedly from random points covering the space
# With this and other algorithms we could modify the
# make_random_search_matrix to use Latin Hypercube sampling to be more efficient at
# covering our parameter space but for our number dimensions this is not a problem
GLOBAL_NM_SEARCH_SIZE = 500
nm_rand_matrix = make_random_search_matrix(GLOBAL_NM_SEARCH_SIZE)
## Do NM on all of our starting parameter sets
nm_gof_info = apply(nm_rand_matrix, 1, nm_score_params_likelihood)
## ##########################
## This Calibration Laboratory is written by Jeremy D. Goldhaber-Fiebert (c) 2019
## It gratefully acknowledges help from Fernando Alarid and materials from Fernando Alarid and Eva Enns
## as well as help from Eline Krijkamp
## ##########################
# STEP 1: PACKAGES THAT NEED TO BE INSTALLED VIA TOOLS ===========
## INCLUDING LIBRARIES THAT HAVE ROUTINES WE WILL NEED FOR OPTIMIZATION
## AND FOR GRAPHING OUR RESULTS
library(nloptr)
library(ggplot2)
library(weights)
library(IMIS)
# SECTION: SOME GLOBALS THAT MAKE OUR CODE MORE READABLE ===========
# The number of years our simulation will run
GLOBAL_YEARS = 29
# The number of health states our model will have
GLOBAL_STATES = 3
# The names of the health states
GLOBAL_STATE_NAMES = c("Healthy", "Sick", "Dead")
# The columns/rows in our transition matrix that correspond to each state
GLOBAL_HEALTHY_COL = 1
GLOBAL_SICK_COL = 2
GLOBAL_DEAD_COL = 3
GLOBAL_HEALTHY_ROW = GLOBAL_HEALTHY_COL
GLOBAL_SICK_ROW = GLOBAL_SICK_COL
GLOBAL_DEAD_ROW = GLOBAL_DEAD_COL
# When we do likelihood based calibration, the amount of information in the
# Three studies whose outcomes we use as targets for calibration
GLOBAL_SAMP_SIZE_STUDY1 = 100
GLOBAL_SAMP_SIZE_STUDY2 = 100
GLOBAL_SAMP_SIZE_STUDY3 = 100
# SECTION: UTILITY FUNCTIONS FOR GRAPHING AND DESCRIPTIVES ==========
## Utility function
summarize_outcomes <- function(outcomes) {
# This function summarized the results stored in the outcome matrix and gives the mean and the minimum and maximum value of the variables stored in columns 1, 3 and 4.
# columns 1, 3, 4 correspond to our 3 calibrated parameters:
# column 1: probability of Healthy -> Sick
# column 3: probability of Sick -> Healthy
# column 4: hazard rate ration of death for Sick people relative to Healthy people
print(paste(mean(outcomes[, 1]),  "{", min(outcomes[, 1]), ",",  max(outcomes[, 1]), "}"))
print(paste(mean(outcomes[, 3]),  "{", min(outcomes[, 3]), ",",  max(outcomes[, 3]), "}"))
print(paste(mean(outcomes[, 4]),  "{", min(outcomes[, 4]), ",",  max(outcomes[, 4]), "}"))
}
graph_outcomes <- function(outcomes, gtitle) {
# This function makes a multi-panel plot of posterior marginal distributions of our
# calibrated parameters and also the 2-way distributions (correlation) between the
# calibrated parameters
par(mfrow = c(2, 3), oma = c(0, 0, 2, 0)) # combine multiple plots into one overall graph
plot(density(outcomes[, 1]), main = "Posterior HS")
plot(density(outcomes[, 3]), main = "Posterior SH")
plot(density(outcomes[, 4]), main = "Posterior hrr D in S")
plot(outcomes[, 1], outcomes[, 3], main = "HS to SH")
plot(outcomes[, 1], outcomes[, 4], main = "HS to hrr")
plot(outcomes[, 3], outcomes[, 4], main = "SH to hrr")
mtext(gtitle, outer = TRUE, cex = 1.5)
}
compare_results <- function(gf1, gf2, gtitle) {
# This function is very similar to graph results except that it shows
# the marginal distributions for the posterior/calibrated parameters for
# types of calibration
# The top row of plots shows it for the first calibration approach
# The bottom row of plots shows it for the second calibration approach
par(mfrow = c(2, 3))
plot(density(gf1[, 1]), main = "Posterior HS")
plot(density(gf1[, 3]), main = "Posterior SH")
plot(density(gf1[, 4]), main = "Posterior hrr D in S")
plot(density(gf2[, 1]), main = "Posterior HS")
plot(density(gf2[, 3]), main = "Posterior SH")
plot(density(gf2[, 4]), main = "Posterior hrr D in S")
mtext(gtitle, outer = TRUE, cex = 1.5)
}
# STEP 2: IMPLEMENTING THE MARKOV MODEL ==========
## BEGIN: MARKOV MODEL WE ARE CALIBRATING
init_state <- function(propH, propS){
# At the start of the Markov mode, we must initialize
# The proportion of our starting cohort in each of the states
# The function assumes noone starts in the Dead health state
state <- matrix(nrow = 1, ncol = 3, dimnames = list(1, GLOBAL_STATE_NAMES))
state[GLOBAL_HEALTHY_COL] = propH
state[GLOBAL_SICK_COL] = propS
state[GLOBAL_DEAD_COL] = 1 - (propH + propS)
return(state)
}
init_transitions <- function(pHS, pHD, pSH, pSD){
# The function builds a transition matrix of the appropriate size GLOBAL_STATES X GLOBAL_STATES
# It fills it with the transitions we pass in
# Note: It assumes that the passed-in probability of transition between non-dead health states is conditional on not dying
# Note: It makes sure that row probabilities sum to 1 and are always within the range [0, 1]
# Note: Dead is an absorbing state
transitions <- matrix(nrow = GLOBAL_STATES, ncol = GLOBAL_STATES, dimnames = list(GLOBAL_STATE_NAMES, GLOBAL_STATE_NAMES))
transitions[GLOBAL_HEALTHY_ROW, GLOBAL_HEALTHY_COL] = min(max(0, ((1 - pHD) * (1 - pHS))), 1)
transitions[GLOBAL_HEALTHY_ROW, GLOBAL_SICK_COL] = (1 - pHD) * (pHS)
transitions[GLOBAL_HEALTHY_ROW, GLOBAL_DEAD_COL] = pHD
transitions[GLOBAL_SICK_ROW, GLOBAL_HEALTHY_COL] = (1 - pSD) * (pSH)
transitions[GLOBAL_SICK_ROW, GLOBAL_SICK_COL] = min(max(0, ((1 - pSD) * (1 - pSH))), 1)
transitions[GLOBAL_SICK_ROW, GLOBAL_DEAD_COL] = pSD
transitions[GLOBAL_DEAD_ROW, GLOBAL_HEALTHY_COL] = 0
transitions[GLOBAL_DEAD_ROW, GLOBAL_SICK_COL] = 0
transitions[GLOBAL_DEAD_ROW, GLOBAL_DEAD_COL] = 1
return(transitions)
}
run_markov <- function(pHS, pHD, pSH, hrrSD) {
# This function builds the initial cohort distribution between states
# Then it constructs the appropriate transition matrix
# Then it runs the model for the number of years (cycles)
# It returns the trace of the model run with the first row being the starting distribution
# setup the initial state (all in H)
state <- init_state(propH = 1, propS = 0)
# setup the transition matrix.
# in this model, transition probabilities are not time varying
rate_death_H = -1 * log(1 - pHD)
transitions <- init_transitions(pHS, pHD, pSH, 1 - exp(-1 * rate_death_H * hrrSD))
# initialize the trace to the starting state
trace_matrix <- matrix(nrow = GLOBAL_YEARS, ncol = 3, dimnames = list(paste("year", 1:GLOBAL_YEARS, sep = " "), GLOBAL_STATE_NAMES))
trace_matrix[1, ] = state
# run the model for GLOBAL_YEARS using annual cycles
# record the state after each cycle
for (year in 2:GLOBAL_YEARS){
trace_matrix[year, ] = trace_matrix[year - 1, ] %*% transitions
}
return(trace_matrix)
}
## END: MARKOV MODEL WE ARE CALIBRATING
# STEP 3: TESTING THE MARKOV MODEL ================
## Run the model with pHS=0.1, pHD=0.01, pSH=0.05, hrrSD=8.0
## plot the fraction of the original cohort in H over time
## plot the fraction of the original cohort in S over time
## determine fractions in H and S at appropriate time points
out_test1 = run_markov(pHS = 0.1, pHD = 0.01, pSH = 0.05, hrrSD = 8.0)
# SHOWING OUR TRACE GRAPHICALLY
plot( out_test1[, GLOBAL_HEALTHY_COL], type = "l", col = "blue")
lines(out_test1[, GLOBAL_SICK_COL], col = "red")
lines(out_test1[, GLOBAL_DEAD_COL], col = "black")
# SHOWING OUR TRACE GRAPHICALLY
# WE CAN GET FRACTION OF ORIGINAL COHORT ALIVE AT STEP 12 AND HEALTHY
# WE CAN GET FRACTION OF ORIGINAL COHORT ALIVE AT STEP 29 AND SICK
out_test1[11,  GLOBAL_HEALTHY_COL]
out_test1[28, GLOBAL_SICK_COL]
## Rerun the model with pHS=0.3, pHD=0.01, pSH=0.05, hrrSD=8.0
## plot the fraction of the original cohort in H over time
## plot the fraction of the original cohort in S over time
## determine fractions in H and S at appropriate time points
out_test2 = run_markov(pHS = 0.3, pHD = 0.01, pSH = 0.05, hrrSD = 8.0)
# SHOWING OUR TRACE GRAPHICALLY
plot( out_test2[, GLOBAL_HEALTHY_COL], type = "l", col = "blue")
lines(out_test2[, GLOBAL_SICK_COL], col = "red")
lines(out_test2[, GLOBAL_DEAD_COL], col = "black")
# SHOWING OUR TRACE GRAPHICALLY
# WE CAN GET FRACTION OF ORIGINAL COHORT ALIVE AT STEP 12 AND HEALTHY
# WE CAN GET FRACTION OF ORIGINAL COHORT ALIVE AT STEP 29 AND SICK
out_test2[11, GLOBAL_HEALTHY_COL]
out_test2[28, GLOBAL_SICK_COL]
# SECTION: UTILITY FUNCTIONS FOR RANDOM SEARCH CALIBRATION ==========
run_calibration <- function(ssize, score_function) {
# This funciton takes a number of samples (equal to ssize) from our prior uncertainty
# distributions of our parameters
# It then applies (runs) our Markov model with each set of parameters and
# with the trace from each of the model runs makes a comparison to our
# Calibration targets using the score function
# It returns a matrix each row corresponding to the sampled parameters and the score they received
# A better (lower) score indicates a better fit to the calibration targets
rand_matrix = make_random_search_matrix(ssize)
gof_score = apply(rand_matrix, 1, score_function)
return(cbind(rand_matrix, gof_score))
}
acceptance_sample <- function(ss, num_accepted) {
# One typical approach in random search calibration is to
# take a certain number (num_accepted) of the best fitting parameter sets contained in the matrix ss
# and to treat these best "accepted" parameters as the posterior with each
# of the good-fitting sets having equal likelihood
# The converts the number we want to accept (num_accepted) into a percentile
# selects those parameter sets with scores below (better than) this critical percentile
# It then returns the parameter sets and their scores that make the posterior
my_percentile = num_accepted / length(ss[, 5])
threshold_value = quantile(ss[, 5], probs = c(my_percentile))
good_fits = ss[ss[, 5] <= threshold_value, ]
return(good_fits)
}
describe_calibration_results <- function(good_fits, gtitle) {
# This function is a utility function that prints out some information about the good fitting sets
# The percentiles of scores that we are treating as uniform in our posterior
# The means and uncertainty intervals of the scores
# The best score
# It also produces a multi-panel graph of the posterior distributions
print(quantile(good_fits[, 5], probs = c(0, 0.0025, 0.005, 0.01, 0.025, 0.10, 0.50, 0.90)))
summarize_outcomes(good_fits)
print(good_fits[good_fits[, 5] == min(good_fits[, 5]), ])
graph_outcomes(good_fits, gtitle)
}
# STEP 4: RANDOM SEARCH CALIBRATION WITH SUM OF SQUARES GOF ==========
## RANDOM SEARCH CALIBRATION
## We set up the matrix that contains our priors
## We will assume that we know pHD for certain (pHD=0.005) and
## that for the probabilities we have uniform priors from [0, 0.5] and
## that for the hazard rate ratio of SD to HD we have a uniform prior from [0, 10].
## The first step is to set up a matrix of samples from these prior.
## We will generate 20,000 samples. Set up the matrix.
make_random_search_matrix <- function(search_size) {
rand_matrix <- matrix(nrow = search_size, ncol = 4) # THIS FOUR SHOULD BE ABLE TO CHANGE - THIS IS CASE SPECIFIC
rand_matrix[, 1] = runif(search_size)/2
rand_matrix[, 2] = 0.005
rand_matrix[, 3] = runif(search_size)/2
rand_matrix[, 4] = 1 + (9 * runif(search_size))
summarize_outcomes(rand_matrix)
return(rand_matrix)
}
compute_gof_sse <- function(trace_matrix) { # IDEA TO LOAD THE DATA FROM EXCEL -> USE NAMES
# This function applies a sum of square errors scoring function to our Markov model's trace
# comparing it to our calibration targets which we have from our 3 studies
# See the Excel sheet/other documentation for those studies
#
# For example gof1 compares the prevalence of Sick at year 2 to
# to the equivalent value we compute from our Markov model trace at year 2
# The difference in target prevalence and model-predicted prevalence is then
# squared to produce the sum of squared errors component for this target
#
# We take the sum of the components to get the total sum of squared errors (SSE)
# Better fitting parameter sets will have lower/smaller SSEs
gof1 = (0.145 - trace_matrix[3,  GLOBAL_SICK_COL]/(1 - trace_matrix[3,  GLOBAL_DEAD_COL]))^2
gof2 = (0.246 - trace_matrix[5,  GLOBAL_SICK_COL]/(1 - trace_matrix[5,  GLOBAL_DEAD_COL]))^2
gof3 = (0.318 - trace_matrix[7,  GLOBAL_SICK_COL]/(1 - trace_matrix[7,  GLOBAL_DEAD_COL]))^2
gof4 = (0.368 - trace_matrix[9,  GLOBAL_SICK_COL]/(1 - trace_matrix[9,  GLOBAL_DEAD_COL]))^2
gof5 = (0.479 - trace_matrix[21, GLOBAL_SICK_COL]/(1 - trace_matrix[21, GLOBAL_DEAD_COL]))^2
gof6 = (0.048 - trace_matrix[7,  GLOBAL_DEAD_COL])^2
gof7 = (0.140 - trace_matrix[15, GLOBAL_DEAD_COL])^2
gof8 = (0.211 - trace_matrix[21, GLOBAL_DEAD_COL])^2
gof9 = (0.266 - trace_matrix[26, GLOBAL_DEAD_COL])^2
gof10 = (0.785 - trace_matrix[4,  GLOBAL_HEALTHY_COL])^2
gof11 = (0.540 - trace_matrix[11, GLOBAL_HEALTHY_COL])^2
gof12 = (0.449 - trace_matrix[17, GLOBAL_HEALTHY_COL])^2
gof13 = (0.403 - trace_matrix[22, GLOBAL_HEALTHY_COL])^2
gof14 = (0.381 - trace_matrix[25, GLOBAL_HEALTHY_COL])^2
gof = sum(gof1, gof2, gof3, gof4, gof5, gof6, gof7, gof8, gof9, gof10, gof11, gof12, gof13, gof14)
return(gof)
}
score_params_gof <- function(params) {
# This function produces a SSE goodness of fit score for any parameter set
# By running the model and then applying our scoring function to the trace produced by the model
pHS   = params[1]
pHD   = params[2]
pSH   = params[3]
hrrSD = params[4]
trace_matrix = run_markov(pHS, pHD, pSH, hrrSD)
gof = compute_gof_sse(trace_matrix)
return(gof)
}
# STEP 5: TEST SSE GOF CALIBRATION ===========
# We start with 5000 uniform random samples from the prior distributions of our parameters
samples_scores = run_calibration(5000, score_params_gof)
# Let's describe the calibration: what is the distributions of the SSE scores we achieved (lower is better)
quantile(samples_scores[, 5], probs = c(0, 0.0025, 0.005, 0.01, 0.025, 0.10, 0.50, 0.90))
# A common approach is to do acceptance sampling
# choose the top Xth percentile of parameter sets based on their score
# Let's take the top X% of scores (5,000 samples where we take the top 200)
rand_good_fits = acceptance_sample(samples_scores, 200)
describe_calibration_results(rand_good_fits, "Random SSE GOF 5000 samples")
# How much better can we do with 100000 samples?
samples_scores_2 = run_calibration(100000, score_params_gof)
rand_good_fits_2 = acceptance_sample(samples_scores_2, 1000)
describe_calibration_results(rand_good_fits_2, "Random SSE GOF 100000 samples")
### Finally we compare our results from 5000 to 100000
compare_results(rand_good_fits, rand_good_fits_2, "Comparing Random SSE calibration 5000 to 100000 samples")
# STEP 6: RANDOM SEARCH CALIBRATION WITH LIKELIHOOD-BASED GOF ==========
# Instead of using an SSE-based calibration which assumes symmetry and focuses on the observed mean
# We can use likelihood-based calibration which uses beta distributions to describe the targets
# They do not assume symmetry around the observed mean, respect the [0-1] range of the outcomes
# and appropriately map study sample size into uncertainty ranges (how quickly the likelihood gets small)
compute_gof_likelihood <- function(trace_matrix) {  #WE NEED TO CHANGE THIS FUNCTION => VERY UGLY
## The conservative scale parameter is used for the following reason:
## I am assuming that because we don't know how many people die to not be inlcuded in the prevalence denominators
## we will use the maximum hrr (10) and our HD rate and kill off this many people
## Also assuming no correlation in the targets over time other than through the model
## Because our optimization/calibration is doing minimization
## and because the likelihoods can get very big and very small,
## We actually will compute the log-likelihoods
## and then sum them and multiply them by -1
## maximizing the log-likelihood (or minizing -1 * log-likelihood)
## will get the same point as doing so in the likelihood space itself
conservative_scale = exp(-1 * 10 * -1 * log(1 - 0.005) * 2)
gof1 = log(dbeta(trace_matrix[3, GLOBAL_SICK_COL]/(1 - trace_matrix[3, GLOBAL_DEAD_COL]), 0.145 * GLOBAL_SAMP_SIZE_STUDY1 * conservative_scale, (1 - 0.145) * GLOBAL_SAMP_SIZE_STUDY1 * conservative_scale))
conservative_scale = exp(-1 * 10 * -1 * log(1 - 0.005) * 4)
gof2 = log(dbeta(trace_matrix[5, GLOBAL_SICK_COL] /(1 - trace_matrix[5, GLOBAL_DEAD_COL]), 0.246 * GLOBAL_SAMP_SIZE_STUDY1*conservative_scale, (1 - 0.246) * GLOBAL_SAMP_SIZE_STUDY1 * conservative_scale))
conservative_scale = exp(-1 * 10 * -1 * log(1 - 0.005) * 6)
gof3 = log(dbeta(trace_matrix[7, GLOBAL_SICK_COL]/(1 - trace_matrix[7, GLOBAL_DEAD_COL]), 0.318 * GLOBAL_SAMP_SIZE_STUDY1 * conservative_scale, (1 - 0.318) * GLOBAL_SAMP_SIZE_STUDY1 * conservative_scale))
conservative_scale = exp(-1 * 10 * -1 * log(1 - 0.005) * 8)
gof4 = log(dbeta(trace_matrix[9, GLOBAL_SICK_COL]/(1 - trace_matrix[9, GLOBAL_DEAD_COL]), 0.368 * GLOBAL_SAMP_SIZE_STUDY1 * conservative_scale, (1 - 0.368) * GLOBAL_SAMP_SIZE_STUDY1 * conservative_scale))
conservative_scale = exp(-1 * 10 * -1 * log(1 - 0.005) * 20)
gof5 = log(dbeta(trace_matrix[21, GLOBAL_SICK_COL]/(1 - trace_matrix[21, GLOBAL_DEAD_COL]), 0.479 * GLOBAL_SAMP_SIZE_STUDY1 * conservative_scale, (1 - 0.479) * GLOBAL_SAMP_SIZE_STUDY1 * conservative_scale))
gof6  = log(dbeta(trace_matrix[7,  GLOBAL_DEAD_COL], 0.048 * GLOBAL_SAMP_SIZE_STUDY2, (1 - 0.048) * GLOBAL_SAMP_SIZE_STUDY2))
gof7  = log(dbeta(trace_matrix[15, GLOBAL_DEAD_COL], 0.140 * GLOBAL_SAMP_SIZE_STUDY2, (1 - 0.140) * GLOBAL_SAMP_SIZE_STUDY2))
gof8  = log(dbeta(trace_matrix[21, GLOBAL_DEAD_COL], 0.211 * GLOBAL_SAMP_SIZE_STUDY2, (1 - 0.211) * GLOBAL_SAMP_SIZE_STUDY2))
gof9  = log(dbeta(trace_matrix[26, GLOBAL_DEAD_COL], 0.266 * GLOBAL_SAMP_SIZE_STUDY2, (1 - 0.266) * GLOBAL_SAMP_SIZE_STUDY2))
gof10 = log(dbeta(trace_matrix[4,  GLOBAL_HEALTHY_COL], 0.785 * GLOBAL_SAMP_SIZE_STUDY3, (1 - 0.785) * GLOBAL_SAMP_SIZE_STUDY3))
gof11 = log(dbeta(trace_matrix[11, GLOBAL_HEALTHY_COL], 0.540 * GLOBAL_SAMP_SIZE_STUDY3, (1 - 0.540) * GLOBAL_SAMP_SIZE_STUDY3))
gof12 = log(dbeta(trace_matrix[17, GLOBAL_HEALTHY_COL], 0.449 * GLOBAL_SAMP_SIZE_STUDY3, (1 - 0.449) * GLOBAL_SAMP_SIZE_STUDY3))
gof13 = log(dbeta(trace_matrix[22, GLOBAL_HEALTHY_COL], 0.403 * GLOBAL_SAMP_SIZE_STUDY3, (1 - 0.403) * GLOBAL_SAMP_SIZE_STUDY3))
gof14 = log(dbeta(trace_matrix[25, GLOBAL_HEALTHY_COL], 0.381 * GLOBAL_SAMP_SIZE_STUDY3, (1 - 0.381) * GLOBAL_SAMP_SIZE_STUDY3))
gof = -1 * sum(gof1, gof2, gof3, gof4, gof5, gof6, gof7, gof8, gof9, gof10, gof11, gof12, gof13, gof14)
return(gof)
}
score_params_likelihood <- function(params) {
# This function produces a likelihood-based goodness of fit score for any parameter set
# by running the model and then applying our scoring function to the trace produced by the model
pHS = params[1]
pHD = params[2]
pSH = params[3]
hrrSD = params[4]
trace_matrix = run_markov(pHS, pHD, pSH, hrrSD)
gof = compute_gof_likelihood(trace_matrix)
return(gof)
}
# STEP 7: TEST LIKELIHOOD-BASED GOF CALIBRATION ===========
# We start with 5000 uniform random samples
samples_scores_3 = run_calibration(5000, score_params_likelihood)
# Let's describe the calibration, specifically the distribution of the -1*log-likelihood (lower is better)
quantile(samples_scores_3[,5], probs = c(0, 0.0025, 0.005, 0.01, 0.025, 0.10, 0.50, 0.90))
# A common approach is to do acceptance sampling
# choose the top Xth percentile of parameter sets based on their score
# Let's take the top X% of scores (5,000 samples where we take the top 200)
rand_good_fits_3 = acceptance_sample(samples_scores_3, 200)
describe_calibration_results(rand_good_fits_3,"Random Likelihood-based GOF 5000 samples")
# How much better can we do with 100000 samples?
samples_scores_4 = run_calibration(100000, score_params_likelihood)
rand_good_fits_4 = acceptance_sample(samples_scores_4, 1000)
describe_calibration_results(rand_good_fits_4, "Random Likelihood-based GOF 100000 samples")
### Finally we compare our results from 5000 to 100000
compare_results(rand_good_fits_3, rand_good_fits_4, "Comparing Random Likeilhood-based calibration 5000 to 100000 samples")
compare_results(rand_good_fits,   rand_good_fits_3, "Comparing 5000 to 5000 SSE to Likelihood")
compare_results(rand_good_fits_2, rand_good_fits_4, "Comparing 100000 to 100000 SSE to Likelihood")
# STEP 8: SIR CALIBRATION: EXTENSION OF RANDOM SEARCH =====================================
## SIR (Sampling importance resampling) is a nice alternative to acceptance sampling
## it can start with samples from the prior
## then compute likelihoods given some data
## and then resample based on the normalized likelihood
## Our simple example assumes an informative uniform prior
## given this the likelihood tells us what we want
## since we have the negative log-likelihood
## we transform this to be the likelihood for weighting purposes (weights = www)
www = 1/(exp(samples_scores_4[, 5]))
## We normalize the weights by diviing each weight by the sum of the weights
www = www/sum(www)
## in our simple example we just show the posterior distributions of our parameters with the weights
## we can use all of our large sample to get a smooth posterior
## vs. with acceptance sampling where we have only the number we accepted and therefore assume
## all values outside that range have probability 0 of occuring in our posterior and that
## within our posteiror all samples are equally likely
## in reality we could do the update formally on the prior to get the posterior likelihood (normalized)
## and then attach a column to our samples to resample arbitrarily large numbers of samples from our posterior
par(mfrow= c (2, 3))
plot(density(x = samples_scores_4[, 1], weights = www), main = "Posterior HS")
plot(density(x = samples_scores_4[, 3], weights = www), main = "Posterior SH")
plot(density(x = samples_scores_4[, 4], weights = www), main = "Posterior hrr D in S")
plot(density(x = rand_good_fits_4[, 1]), main = "Posterior HS")
plot(density(x = rand_good_fits_4[, 3]), main = "Posterior SH")
plot(density(x = rand_good_fits_4[, 4]), main = "Posterior hrr D in S")
mtext("Results of SIR Calibration", outer = TRUE, cex = 1.5)
# STEP 9: IMIS CALIBRATION: EXTENSION OF SIR =============================================
## We are using a particular implementation of the IMIS algorithm in R for our
## IMIS calibration example: https://cran.r-project.org/web/packages/IMIS/index.html
##
## It requires that we implement several functions (likelihood, prior, and sample.prior)
## Below I will descirbe what those functions are doing and how/why we map our other
## calibration functions to them
likelihood <- function(theta) {
# The IMIS algorith sometimes uses matrices and sometimes uses vectors
# for evaluating the likelihood of a parameter sample so we implement
# two parallel versions of the code
# First since in our example one of our parameters (probability of Healthy -> Dead is fixed at 0.005)
# We put it in (it is not getting calibrated really but needed to run our model)
# Then we run the model for each parameter set and compute the likelihood
# Because IMIS uses a multivariate normal approximation to enrich samples near
# the best fitting parameter set, it is possible to go outside of our parameter bounds
# We therefore give a 0 likelihood to any parameter sets that do this
if (is.matrix(theta)) {
fixed_col = rep(0.005, length(theta[, 1]))
theta_one = cbind(theta[, 1], fixed_col, theta[, 2], theta[, 3])
gof_score = 1/exp(apply(theta_one, 1, score_params_likelihood))
gof_score[theta_one[, 1] < 0]   = 0
gof_score[theta_one[, 1] > 0.5] = 0
gof_score[theta_one[, 3] < 0]   = 0
gof_score[theta_one[, 3] > 0.5] = 0
gof_score[theta_one[, 4] < 1]   = 0
gof_score[theta_one[, 4] > 10]  = 0
}
else {
theta_one = cbind(theta[1], 0.005, theta[2], theta[3])
gof_score = 1/exp(apply(theta_one, 1, score_params_likelihood))
gof_score[theta_one[1] < 0]   = 0
gof_score[theta_one[1] > 0.5] = 0
gof_score[theta_one[3] < 0]   = 0
gof_score[theta_one[3] > 0.5] = 0
gof_score[theta_one[4] < 1]   = 0
gof_score[theta_one[4] > 10]  = 0
}
return(gof_score)
}
prior <- function(theta) {
## This is arbitrary but we just say that all samples are equally likely but we are uncertain
## so they all have a low likelihood
## we would need to modify this a bit if we wanted to be super correct about
## specifying the prior likelihood but it is good enough for now
if (is.matrix(theta)) {
ppp = theta[, 1]
ppp = rep(1/100000, length(ppp))
}
else {
ppp = 1/100000
}
return(ppp)
}
sample.prior <- function(n) {
## This is directly analogous to our sampling n parameter sets from our prior distributions in
## all of the other calibrations
rand_matrix = make_random_search_matrix(n)
rand_matrix <-rand_matrix[, -2]
return(rand_matrix)
}
## HERE WE RUN THE IMIS
## Our initial samples will be 10*B (By how the IMIS is implemented in the R package)
## Each subsequent sample will have B=5000 parameter sets drawn from near
## the best fitting set at each iteration
## The number of iterations is number_k = 50
## We will get a final resample of 200000
## At none of the iterations will we use built in optimizers (D=0)
## instead the algorithm uses a multivariate normal centered around the best fitting set
## with covariance matrix estimated from nearby samples (mahalanobis distance) weighted by their
## likelihood
result = IMIS(B = 5000, B.re = 200000, number_k = 50, D = 0)
install.packages("truncnorm", dependencies = TRUE)
library(truncnorm)  # load the package truncnorm
betaPar <- function(m, s)  # extract the  parameters of a beta distribution from mean and st. deviation
{ a <- m * ((m * (1 - m) / s ^ 2) - 1)
b <- (1 - m) * ((m * (1 - m) / s ^ 2) - 1)
list(a = a, b = b)
}
